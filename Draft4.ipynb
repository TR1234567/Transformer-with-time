{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TR1234567/Transformer-with-time/blob/main/Draft4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-forecasting\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7onX8iSbkTt",
        "outputId": "970518e2-8838-4d53-db47-878e368a4d67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-forecasting\n",
            "  Downloading pytorch_forecasting-1.0.0-py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.4/140.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.80 (from pytorch-forecasting)\n",
            "  Downloading fastapi-0.109.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning<3.0.0,>=2.0.0 (from pytorch-forecasting)\n",
            "  Downloading lightning-2.1.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (3.7.1)\n",
            "Collecting optuna<4.0.0,>=3.1.0 (from pytorch-forecasting)\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<=3.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.5.3)\n",
            "Collecting pytorch-optimizer<3.0.0,>=2.5.1 (from pytorch-forecasting)\n",
            "  Downloading pytorch_optimizer-2.12.0-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.8/155.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.2.2)\n",
            "Requirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (1.11.4)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (0.14.1)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-forecasting) (2.1.0+cu121)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.80->pytorch-forecasting) (1.10.13)\n",
            "Collecting starlette<0.36.0,>=0.35.0 (from fastapi>=0.80->pytorch-forecasting)\n",
            "  Downloading starlette-0.35.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.8.0 (from fastapi>=0.80->pytorch-forecasting)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (23.2)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n",
            "  Downloading torchmetrics-1.3.0-py3-none-any.whl (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.66.1)\n",
            "Collecting pytorch-lightning (from lightning<3.0.0,>=2.0.0->pytorch-forecasting)\n",
            "  Downloading pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna<4.0.0,>=3.1.0->pytorch-forecasting) (2.0.24)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<=3.0.0,>=1.3.0->pytorch-forecasting) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<=3.0.0,>=1.3.0->pytorch-forecasting) (2023.3.post1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0,>=1.2->pytorch-forecasting) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.0->pytorch-forecasting) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pytorch-forecasting) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pytorch-forecasting) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pytorch-forecasting) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pytorch-forecasting) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pytorch-forecasting) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pytorch-forecasting) (3.1.1)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels->pytorch-forecasting) (0.5.6)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna<4.0.0,>=3.1.0->pytorch-forecasting)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (67.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels->pytorch-forecasting) (1.16.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna<4.0.0,>=3.1.0->pytorch-forecasting) (3.0.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.36.0,>=0.35.0->fastapi>=0.80->pytorch-forecasting) (3.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.0->pytorch-forecasting) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.80->pytorch-forecasting) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.80->pytorch-forecasting) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.36.0,>=0.35.0->fastapi>=0.80->pytorch-forecasting) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>=2022.5.0->lightning<3.0.0,>=2.0.0->pytorch-forecasting) (2023.11.17)\n",
            "Installing collected packages: typing-extensions, Mako, colorlog, starlette, lightning-utilities, torchmetrics, pytorch-optimizer, fastapi, alembic, pytorch-lightning, optuna, lightning, pytorch-forecasting\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.0 alembic-1.13.1 colorlog-6.8.0 fastapi-0.109.0 lightning-2.1.3 lightning-utilities-0.10.0 optuna-3.5.0 pytorch-forecasting-1.0.0 pytorch-lightning-2.1.3 pytorch-optimizer-2.12.0 starlette-0.35.1 torchmetrics-1.3.0 typing-extensions-4.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knnrcn06bt-t",
        "outputId": "6d4977f7-b537-457a-e43f-057933f16494"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "eGTkinoKbVrd",
        "outputId": "7cad9f6b-03e4-49c9-9c7f-0ac2cd6f6261"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import yfinance\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from pytorch_forecasting.models.temporal_fusion_transformer.sub_modules import GatedResidualNetwork\n",
        "import gc\n",
        "from tqdm.auto import tqdm\n",
        "import wandb\n",
        "import math\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model"
      ],
      "metadata": {
        "id": "tVDxfhduc99H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "rD4gxN7NbVrl"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Time2Vector(nn.Module):\n",
        "    def __init__(self, seq_len):\n",
        "        super(Time2Vector, self).__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # Linear part\n",
        "        self.linear_weights = nn.Parameter(torch.Tensor(self.seq_len))\n",
        "        self.linear_bias = nn.Parameter(torch.Tensor(self.seq_len))\n",
        "\n",
        "        # Periodic part\n",
        "        self.periodic_weights1 = nn.Parameter(torch.Tensor(self.seq_len))\n",
        "        self.periodic_bias1 = nn.Parameter(torch.Tensor(self.seq_len))\n",
        "        self.periodic_weights2 = nn.Parameter(torch.Tensor(self.seq_len))\n",
        "        self.periodic_bias2 = nn.Parameter(torch.Tensor(self.seq_len))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.uniform_(self.linear_weights, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.linear_bias, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.periodic_weights1, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.periodic_bias1, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.periodic_weights2, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.periodic_bias2, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Mean reduction across the last dimension\n",
        "        x = torch.mean(x, dim=-1)\n",
        "\n",
        "        # Linear time feature\n",
        "        time_linear = self.linear_weights * x + self.linear_bias\n",
        "        time_linear = time_linear.unsqueeze(-1)\n",
        "\n",
        "        # Periodic time features\n",
        "        time_periodic1 = torch.sin(x * self.periodic_weights1 + self.periodic_bias1)\n",
        "        time_periodic1 = time_periodic1.unsqueeze(-1)\n",
        "        time_periodic2 = torch.sin(x * self.periodic_weights2 + self.periodic_bias2)\n",
        "        time_periodic2 = time_periodic2.unsqueeze(-1)\n",
        "\n",
        "        # Concatenate linear and periodic features\n",
        "        return torch.cat([time_linear, time_periodic1, time_periodic2], dim=-1)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    model2 = PortfolioTransformer(embed_dim=3027,asset_dim_in=11,asset_dim_out=10,nhead=8,num_model=256,batch_size=1)\n",
        "    \"\"\"\n",
        "    def __init__(self, asset_dim_in, num_model,nhead, dropout=0.1,batch_size=1):  # Add input_dim parameter\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.embedding = nn.Linear(asset_dim_in+3, num_model) #asset_dim_in = 11, num_model = 256\n",
        "        self.multihead_attn = nn.MultiheadAttention(num_model, nhead)\n",
        "        self.grn = GatedResidualNetwork(input_size=num_model,hidden_size=num_model,output_size=num_model)\n",
        "        # self.grn = GatedResidualNetwork(d_model, dropout, batch_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(num_model)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.embedding(src)\n",
        "        residual = src\n",
        "        src,_ = self.multihead_attn(src, src, src, attn_mask=src_mask)\n",
        "        src = self.dropout(src)\n",
        "        src = residual + src\n",
        "        src = self.layer_norm(src)\n",
        "\n",
        "        residual = src\n",
        "        src = self.grn(src)\n",
        "        src = self.dropout(src)\n",
        "        src = residual + src\n",
        "        src = self.layer_norm(src)\n",
        "\n",
        "        return src\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, num_model, nhead, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.num_model = num_model\n",
        "        self.masked_mha = nn.MultiheadAttention(num_model, nhead)\n",
        "        self.encoder_attn = nn.MultiheadAttention(num_model, nhead, dropout=dropout)\n",
        "        self.grn = GatedResidualNetwork(input_size=num_model,hidden_size=num_model,output_size=num_model)\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = nn.LayerNorm(num_model)\n",
        "        self.layer_norm2 = nn.LayerNorm(num_model)\n",
        "\n",
        "    def forward(self, tgt, memory, src_mask, tgt_mask):\n",
        "        residual = tgt\n",
        "        # Ensure tgt, memory, src_mask, and tgt_mask have the correct dimensions\n",
        "        tgt = tgt.view(tgt.size(0), -1, self.num_model) # Reshape tgt to match the shape of memory\n",
        "        # print(\"this is tgt shape {}\".format(tgt.shape))\n",
        "        memory = memory.view(memory.size(0), -1, self.num_model)\n",
        "        # print(\"this is memory shape {}\".format(memory.shape))\n",
        "        tgt_mask = torch.ones(1,1)\n",
        "\n",
        "        # Transpose tgt for masked_mha\n",
        "        tgt = tgt.transpose(0, 1)\n",
        "\n",
        "        # Masked multi-head attention within decoder\n",
        "        tgt,_ = self.masked_mha(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
        "        # tgt = self.dropout(tgt)\n",
        "\n",
        "        tgt = self.layer_norm1(residual + tgt)\n",
        "\n",
        "        # Transpose tgt back to original order\n",
        "        tgt = tgt.transpose(0, 1)\n",
        "        # print(tgt.shape)\n",
        "        # Multi-head attention to encoder output\n",
        "\n",
        "        tgt= self.encoder_attn(tgt, tgt, memory, attn_mask=src_mask)\n",
        "        # print(tgt[0].shape)\n",
        "        # print(tgt[1].shape)\n",
        "        # tgt = self.dropout(tgt)\n",
        "\n",
        "        tgt = self.layer_norm2(residual + tgt[0])\n",
        "\n",
        "        # Gated residual network\n",
        "        tgt = self.grn(tgt)\n",
        "        # tgt = self.dropout(tgt)\n",
        "        tgt = self.layer_norm2(tgt + residual)\n",
        "\n",
        "        return tgt\n",
        "\n",
        "class PortfolioLoss(torch.nn.Module):\n",
        "    def __init__(self, cost_rate, risk_free_rate=0.1):\n",
        "        super(PortfolioLoss, self).__init__()\n",
        "        self.cost_rate = cost_rate\n",
        "        self.risk_free_rate = risk_free_rate\n",
        "\n",
        "    def forward(self, weights, returns):\n",
        "        # Ensure weights and returns are in the correct shape\n",
        "        # Weights shape: [batch_size, num_assets]\n",
        "        # Returns shape: [batch_size, num_assets]\n",
        "\n",
        "\n",
        "        # Calculate portfolio returns\n",
        "        # Element-wise multiplication and sum over assets for each instance in the batch\n",
        "        portfolio_returns = torch.sum(weights * returns, dim=1)\n",
        "\n",
        "        # Calculate portfolio volatility (standard deviation of returns)\n",
        "        # You may need to adjust this calculation depending on how your returns are structured\n",
        "        portfolio_volatility = torch.std(portfolio_returns, dim=0)\n",
        "\n",
        "        # Calculate transaction costs\n",
        "        # Assuming weights are adjacent allocations, calculate the change in weights\n",
        "        deltas = torch.abs(weights[1:] - weights[:-1])\n",
        "        transaction_costs = self.cost_rate * torch.sum(deltas, dim=1)\n",
        "\n",
        "        # Calculate Sharpe Ratio\n",
        "        # Note: risk_free_rate is typically a constant like a treasury bond yield\n",
        "        sharpe_ratio = (portfolio_returns.mean() - self.risk_free_rate) / portfolio_volatility\n",
        "\n",
        "        # Subtract transaction costs\n",
        "        sharpe_ratio -= transaction_costs.mean()\n",
        "        print(\"sharpe_ratio : \",sharpe_ratio)\n",
        "        # Negate Sharpe Ratio to minimize this value during training\n",
        "        cumulative_returns = np.cumprod(1 + returns) - 1\n",
        "        peak, trough = np.argmax(np.maximum.accumulate(cumulative_returns) - cumulative_returns), np.argmax(cumulative_returns)\n",
        "        max_drawdown = cumulative_returns[peak] - cumulative_returns[trough]\n",
        "        average_return = portfolio_returns.mean()\n",
        "        calmar = average_return / abs(max_drawdown)\n",
        "        # return -sharpe_ratio , -calmar , -max_drawdown\n",
        "        return -sharpe_ratio\n",
        "\n",
        "class PortfolioTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim,asset_dim_in,asset_dim_out,nhead, num_model,batch_size=1):\n",
        "        super(PortfolioTransformer, self).__init__()\n",
        "        self.time2vec = Time2Vector(embed_dim)\n",
        "        self.encoder = EncoderLayer(asset_dim_in=asset_dim_in,num_model=num_model,nhead=nhead,batch_size=batch_size)\n",
        "        self.decoder = DecoderLayer(num_model=num_model,nhead=nhead)\n",
        "        self.linear = nn.Linear(num_model, asset_dim_out)\n",
        "        self.nhead = nhead\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        time_features = self.time2vec(x)\n",
        "        # print(\"X shape \",x.shape)\n",
        "        x = torch.cat([x, time_features], dim=-1)\n",
        "\n",
        "        src_mask = torch.ones(self.nhead, x.shape[0],x.shape[0], dtype=torch.float32)\n",
        "        # print(\"X shape \",x.shape,\"src_mask shape \",src_mask.shape)\n",
        "\n",
        "        x = self.encoder(x, src_mask,)\n",
        "\n",
        "        x = self.decoder(x, x, src_mask, src_mask)\n",
        "        x = self.linear(x)\n",
        "        x = x.mean(dim=1)\n",
        "        # print(\"This is x before softmax\",x)\n",
        "        weights = torch.nn.functional.softmax(x, dim=-1)\n",
        "        print(\"weight of asset shape\",weights.shape)\n",
        "        # print(\"weight of asset\",weights)\n",
        "        return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "sScyQ6_WdB7L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ohkkJfGTbVrh"
      },
      "outputs": [],
      "source": [
        "Tickers = [\"^GSPC\", \"^IXIC\", \"^TNX\", \"^SPGSCI\", \"^VIX\", \"VTI\"]\n",
        "Crypto = [\"BTC-USD\", \"ETH-USD\", \"BNB-USD\", \"XRP-USD\", \"USDT-USD\"]\n",
        "start=\"2012-01-01\"\n",
        "end=\"2022-12-31\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIL_ytZYbVrh",
        "outputId": "a9ce4398-a7cc-4823-83e0-f850c3ebc102"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  6 of 6 completed\n",
            "[*********************100%%**********************]  5 of 5 completed\n"
          ]
        }
      ],
      "source": [
        "Stock_df = yfinance.download(Tickers, start=start, end=end, interval=\"1d\")\n",
        "Crypto_df = yfinance.download(Crypto, start=start, end=end, interval=\"1d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VNAZUMFhbVrh"
      },
      "outputs": [],
      "source": [
        "def clean_stock(df,col):\n",
        "    df = df.fillna(0)\n",
        "    return df[col]\n",
        "def clean_crypto(df,col):\n",
        "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
        "    return df[col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ktOh_p1PbVrh"
      },
      "outputs": [],
      "source": [
        "Stock_df = clean_stock(Stock_df,\"Adj Close\")\n",
        "Crypto_df = clean_crypto(Crypto_df,\"Adj Close\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "573m1irqbVri",
        "outputId": "bf2d9874-de7c-42dd-ce51-d5e88dff1b01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%%**********************]  6 of 6 completed\n",
            "[*********************100%%**********************]  5 of 5 completed\n"
          ]
        }
      ],
      "source": [
        "Stock_df_pred = yfinance.download(Tickers, start=end,  interval=\"1d\")\n",
        "Crypto_df_pred = yfinance.download(Crypto, start=end,  interval=\"1d\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "A2Wxz8RQbVri"
      },
      "outputs": [],
      "source": [
        "Stock_df_pred = clean_stock(Stock_df_pred,\"Adj Close\")\n",
        "Crypto_df_pred = clean_crypto(Crypto_df_pred,\"Adj Close\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LhopzpE7bVrj"
      },
      "outputs": [],
      "source": [
        "before = Stock_df[:\"2014-09-17\"]\n",
        "after = Stock_df[\"2014-09-17\":]\n",
        "after_concat = pd.concat([after,Crypto_df],axis=1).fillna(method='bfill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6ZV4CsWSbVrj"
      },
      "outputs": [],
      "source": [
        "test = pd.concat([Stock_df_pred,Crypto_df_pred],axis=1).fillna(method='bfill')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "_xJFWYBpbVrj"
      },
      "outputs": [],
      "source": [
        "mu = before.rolling(1).mean()\n",
        "mu.drop('^VIX',axis=1,inplace=True)\n",
        "returns_tensor = torch.tensor(mu.values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "W69ucYUdbVrk"
      },
      "outputs": [],
      "source": [
        "mu2 = after_concat.rolling(1).mean()\n",
        "mu2.drop('^VIX',axis=1,inplace=True)\n",
        "returns_tensor2 = torch.tensor(mu2.values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "oYPW5Hl7bVrk"
      },
      "outputs": [],
      "source": [
        "mu_test = test.rolling(1).mean()\n",
        "mu_test.drop('^VIX',axis=1,inplace=True)\n",
        "returns_tensor_test = torch.tensor(mu_test.values, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "CZId-KI_bVrl"
      },
      "outputs": [],
      "source": [
        "after_concat_tensor = torch.tensor(after_concat.values, dtype=torch.float32)\n",
        "after_concat_tensor=after_concat_tensor\n",
        "returns_tensor2 = returns_tensor2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "20Dq41jzbVrm"
      },
      "outputs": [],
      "source": [
        "test_tensor = torch.tensor(test.values, dtype=torch.float32)\n",
        "test_tensor = test_tensor\n",
        "returns_tensor_test = returns_tensor_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "returns_tensor2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtKfT7lmdI9y",
        "outputId": "27c3073f-7704-4074-df88-31904ebc82d8"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3027, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKJTnQAjbVrm"
      },
      "source": [
        "# Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "0f24s65MbVrn"
      },
      "outputs": [],
      "source": [
        "config = dict(\n",
        "    epochs=10,\n",
        "    embed_dim=30,\n",
        "    asset_dim_in=11,\n",
        "    classes=10,\n",
        "    cost_rate=0.01,\n",
        "    batch_size=1,\n",
        "    learning_rate=0.005,\n",
        "    dataset=\"Price data\",\n",
        "    architecture=\"Transformer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "EeQlrz_jbVrn"
      },
      "outputs": [],
      "source": [
        "def make(config):\n",
        "\n",
        "    # Make the model\n",
        "    model = PortfolioTransformer(embed_dim=config.embed_dim,asset_dim_in=config.asset_dim_in,asset_dim_out=10,nhead=8,num_model=256,batch_size=1)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "def prepare_input(dataframe):\n",
        "    # Normalize the data\n",
        "    scaler = StandardScaler()\n",
        "    normalized_data = scaler.fit_transform(dataframe.values)\n",
        "\n",
        "\n",
        "    # Convert to tensor\n",
        "    tensor_data = torch.tensor(normalized_data, dtype=torch.float32)\n",
        "\n",
        "    return tensor_data\n",
        "\n",
        "def prepare_output(dataframe):\n",
        "    # Normalize the data\n",
        "    mu = dataframe.rolling(1).mean()\n",
        "    mu.drop('^VIX',axis=1,inplace=True)\n",
        "    scaler = StandardScaler()\n",
        "    normalized_data = scaler.fit_transform(mu.values)\n",
        "\n",
        "\n",
        "    # Convert to tensor\n",
        "    tensor_data = torch.tensor(normalized_data, dtype=torch.float32)\n",
        "\n",
        "    return tensor_data"
      ],
      "metadata": {
        "id": "gmwTGi7xdhOQ"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "VkOZrj6LbVro"
      },
      "outputs": [],
      "source": [
        "def train(model, df,optimizer, config):\n",
        "    wandb.watch(model, log=\"all\", log_freq=10)\n",
        "\n",
        "    window_size = 30  # 30 days for input\n",
        "    prediction_length = 30  # Next 30 days for prediction\n",
        "    total_batches = (len(df) - window_size - prediction_length + 1) * config.epochs\n",
        "    example_ct = 0\n",
        "    batch_ct = 0\n",
        "\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "        for start_idx in range(len(df) - window_size - prediction_length + 1):\n",
        "            end_idx = start_idx + window_size\n",
        "            next_end_idx = end_idx + prediction_length\n",
        "\n",
        "            input_sequence = df.iloc[start_idx:end_idx]\n",
        "            target_sequence = df.iloc[end_idx:next_end_idx]\n",
        "\n",
        "            model_input = prepare_input(input_sequence)\n",
        "            model_target = prepare_output(target_sequence)\n",
        "\n",
        "            loss = train_batch(model_input, model_target, model, optimizer, config.cost_rate)\n",
        "            example_ct += len(model_input)\n",
        "            batch_ct += 1\n",
        "\n",
        "            if ((batch_ct + 1) % 25) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "def train_batch(X, returns_tensor, model, optimizer, cost_rate):\n",
        "    weights = model(X)\n",
        "    optimizer.zero_grad()\n",
        "    loss = PortfolioLoss(cost_rate=cost_rate)(weights, returns_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    gc.collect()\n",
        "    return loss\n",
        "\n",
        "def train_log(loss, example_ct, epoch):\n",
        "    wandb.log({\"epoch\": epoch, \"loss\": loss})\n",
        "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "3oZVF0AnbVro"
      },
      "outputs": [],
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(project=\"Portfolio Transformer\", config=hyperparameters):\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, optimizer = make(config)\n",
        "      # print(model)\n",
        "\n",
        "      # and use them to train the model\n",
        "      train(model,after_concat,optimizer, config)\n",
        "\n",
        "      # and test its final performance\n",
        "      test_model(model, test_tensor, returns_tensor_test)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "gMJfF4qjbVro"
      },
      "outputs": [],
      "source": [
        "def test_model(model, Y, returns_tensor_y):\n",
        "    model.eval()\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        Y, returns_tensor_y = Y, returns_tensor_y\n",
        "        weights = model(Y)\n",
        "        loss= PortfolioLoss(cost_rate=0.01)(weights, returns_tensor_y)\n",
        "        wandb.log({\"test_loss\": loss})\n",
        "\n",
        "\n",
        "    # Save the model in the exchangeable ONNX format\n",
        "    torch.onnx.export(model, Y, \"model.onnx\")\n",
        "    wandb.save(\"model.onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbV4am6ibVrp"
      },
      "outputs": [],
      "source": [
        "model = model_pipeline(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2A8xcScbVrp"
      },
      "outputs": [],
      "source": [
        "#training model2\n",
        "learning_rate = 0.001\n",
        "cost_rate = 0.01\n",
        "embed_dim = after_concat_tensor.shape[0]\n",
        "asset_dim_in = after_concat_tensor.shape[1]\n",
        "# Initialize model\"asset_dim, embed_dim, nhead, num_encoder_layers\"\n",
        "model = PortfolioTransformer(embed_dim=embed_dim,asset_dim_in=asset_dim_in,asset_dim_out=10,nhead=8,num_model=256,batch_size=1)\n",
        "# Define optimizer using weight from model1\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "#create checkpoint and callback\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_loss',\n",
        "    dirpath='./',\n",
        "    filename='model-{epoch:02d}-{val_loss:.2f}',\n",
        "    save_top_k=3,\n",
        "    mode='min',\n",
        ")\n",
        "# Training loop and add checkpoint and callback\n",
        "for epoch in range(100):\n",
        "\n",
        "    weights = model(after_concat_tensor)\n",
        "    # print(\"thsi is weight output\".format(weights))\n",
        "    # Calculate sharpe ratio loss\n",
        "    # Backpropagate and update parameters\n",
        "    optimizer.zero_grad()\n",
        "    loss = PortfolioLoss(cost_rate=cost_rate)(weights, returns_tensor2)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu6jHtRObVrp",
        "outputId": "82948268-8497-4ef4-d362-077f1cf0f4f6"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Time2Vector.__init__() takes 1 positional argument but 2 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[207], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat([time_linear, time_periodic1, time_periodic2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTime2Vector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m800\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Assuming x is your test set with shape (batch_size, sequence_length)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m x_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m600\u001b[39m))\n",
            "\u001b[0;31mTypeError\u001b[0m: Time2Vector.__init__() takes 1 positional argument but 2 were given"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Time2Vector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Time2Vector, self).__init__()\n",
        "\n",
        "        # Linear part\n",
        "        self.linear_weights = nn.Parameter(torch.Tensor(1))\n",
        "        self.linear_bias = nn.Parameter(torch.Tensor(1))\n",
        "\n",
        "        # Periodic part\n",
        "        self.periodic_weights1 = nn.Parameter(torch.Tensor(1))\n",
        "        self.periodic_bias1 = nn.Parameter(torch.Tensor(1))\n",
        "        self.periodic_weights2 = nn.Parameter(torch.Tensor(1))\n",
        "        self.periodic_bias2 = nn.Parameter(torch.Tensor(1))\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.uniform_(self.linear_weights, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.linear_bias, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.periodic_weights1, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.periodic_bias1, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.periodic_weights2, -0.1, 0.1)\n",
        "        nn.init.uniform_(self.periodic_bias2, -0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Mean reduction across the last dimension\n",
        "        x = torch.mean(x, dim=-1)\n",
        "\n",
        "        seq_len = x.shape[-1]\n",
        "\n",
        "        # Linear time feature\n",
        "        time_linear = self.linear_weights * x + self.linear_bias\n",
        "        time_linear = time_linear.unsqueeze(-1)\n",
        "\n",
        "        # Periodic time features\n",
        "        time_periodic1 = torch.sin(x * self.periodic_weights1 + self.periodic_bias1)\n",
        "        time_periodic1 = time_periodic1.unsqueeze(-1)\n",
        "        time_periodic2 = torch.sin(x * self.periodic_weights2 + self.periodic_bias2)\n",
        "        time_periodic2 = time_periodic2.unsqueeze(-1)\n",
        "\n",
        "        # Concatenate linear and periodic features\n",
        "        return torch.cat([time_linear, time_periodic1, time_periodic2], dim=-1)\n",
        "\n",
        "# Example usage\n",
        "model = Time2Vector()\n",
        "\n",
        "# Assuming x is your test set with shape (batch_size, sequence_length)\n",
        "x_test = torch.rand((10, 600))\n",
        "\n",
        "# Forward pass\n",
        "output = model(x_test)\n",
        "print(output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-KP_EGibVrq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}